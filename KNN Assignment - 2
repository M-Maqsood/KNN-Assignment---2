
Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?

Euclidean distance is the shortest path between source and destination. but Manhattan distance is sum of all the real distances between source(s) and destination(d). This difference affects in the prediction of new data.

Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?
Square Root Method: Take square root of the number of samples in the training dataset.

Cross Validation Method: We should also use cross validation to find out the optimal value of K in KNN. Start with K=1, run cross validation (5 to 10 fold), measure the accuracy and keep repeating till the results become consistent.

K=1, 2, 3... As K increases, the error usually goes down, then stabilizes, and then raises again. Pick the optimum K at the beginning of the stable zone. This is also called Elbow Method.

Domain Knowledge also plays a vital role while choosing the optimum value of K.

K should be an odd number.

Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?
Actually, it depends on the problem and dataset to use in KNN. Each distance metric has a specific area. Euclidean distance, Manhattan Distance, and Minkowski Distance can be used for continous variables. Hamming Distance, Dice Distance, and Jaccard Similarity can be used for categorical variables. Cosine similarity can be used for text data. If you are unsure then you can use cross-validation with different matrics and see which metrics gives high performance.

Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?
Things to keep in mind when performing turning:

Understand the parameters: The main hyperparameter to tune in k-nearest neighbors is k, the number of neighbors to consider. Other parameters include distance metrics, weights, and algorithm types.
Select a distance metric: Choose the right distance metric to measure the similarity between the data points. Common distance metrics include Euclidean, Manhattan, and cosine distance.
Select an appropriate value for k: Selecting a value for k is crucial in k-nearest neighbors. A larger value of k provides a smoother decision boundary but may not be suitable for all datasets. A smaller value of k may lead to overfitting.
Choose an algorithm type: k-nearest neighbors has two algorithm types: brute-force and tree-based. Brute-force algorithm computes the distances between all pairs of points in the dataset while tree-based algorithm divides the dataset into smaller parts.
Cross-validation: Cross-validation is a technique used to validate the performance of the model. It involves splitting the dataset into training and testing sets and evaluating the model's performance on the testing set.
Grid search: Grid search is a hyperparameter tuning technique that involves testing a range of values for each hyperparameter to find the best combination of values.
Random search: Random search is another hyperparameter tuning technique that randomly selects a combination of hyperparameter values to test.
Bias-variance tradeoff: k-nearest neighbors is prone to overfitting due to the high variance in the model. Regularization techniques such as L1 and L2 regularization can be used to mitigate overfitting.
Data preprocessing: Data preprocessing plays a crucial role in k-nearest neighbors. Scaling the data using techniques such as normalization and standardization can improve the model's performance. Outlier removal and feature selection can also help improve the model's performance.
Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?
The KNN algorithm does not work well with large datasets. The cost of calculating the distance between the new point and each existing point is huge, which degrades performance. Feature scaling (standardization and normalization) is required before applying the KNN algorithm to any dataset. Otherwise, KNN may generate wrong predictions.

Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?
It's main disadvantages are that it is quite computationally inefficient and its difficult to pick the “correct” value of K. However, the advantages of this algorithm is that it is versatile to different calculations of proximity, it's very intuitive and that it's a memory based approach.
